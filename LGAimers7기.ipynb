{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtSma7kBhMLL"
      },
      "outputs": [],
      "source": [
        "import os, re, gc, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================\n",
        "# ì„¤ì • ë° ê²½ë¡œ\n",
        "# =========================\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ğŸ”§ ê²½ë¡œ ì„¤ì •\n",
        "BASE_PATH = \"/content/drive/MyDrive/aimers\"\n",
        "TRAIN_PATH = f\"{BASE_PATH}/train/train.csv\"\n",
        "TEST_DIR = f\"{BASE_PATH}/test\"\n",
        "SAMPLE_SUB_PATH = f\"{BASE_PATH}/sample_submission.csv\"\n",
        "OUTPUT_PATH = f\"{BASE_PATH}/submission_test.csv\"\n",
        "\n",
        "# ì—°íšŒì¥ ì²˜ë¦¬ ëª¨ë“œ: \"together\" (ì—°íšŒì¥ í¬í•¨ ì²˜ë¦¬) | \"separate\" (ì—°íšŒì¥ ë”°ë¡œ ì²˜ë¦¬)\n",
        "BANQUET_MODE = \"separate\"\n",
        "\n",
        "# ì €ì¥ íŒŒì¼ ê²½ë¡œ (ì—°íšŒì¥ íƒì§€ ì„¤ì •)\n",
        "BANQUET_DICT_PATH = f\"{BASE_PATH}/banquet_detection_dictionary.json\"\n",
        "BANQUET_THRESH_PATH = f\"{BASE_PATH}/banquet_thresholds.json\"\n",
        "\n",
        "print(f\"ğŸ”§ Base Path: {BASE_PATH}\")\n",
        "print(f\"ğŸ¢ Banquet Mode: {BANQUET_MODE}\")\n",
        "\n",
        "# =========================\n",
        "# ê³µíœ´ì¼ ì²˜ë¦¬ (í´ë°± í¬í•¨)\n",
        "# =========================\n",
        "try:\n",
        "    import holidays\n",
        "    _HAS_HOLIDAYS = True\n",
        "    print(\"ğŸ“… Holidays ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥\")\n",
        "except Exception:\n",
        "    _HAS_HOLIDAYS = False\n",
        "    print(\"âš ï¸ Holidays ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, ê¸°ë³¸ ê³µíœ´ì¼ ì‚¬ìš©\")\n",
        "\n",
        "def _is_holiday_date(d):\n",
        "    if _HAS_HOLIDAYS:\n",
        "        KR_HOLIDAYS = holidays.KR(years=[2023, 2024, 2025])\n",
        "        return d in KR_HOLIDAYS\n",
        "    BASIC_HOLI = {(m, d) for m, d in [(1,1),(3,1),(5,5),(6,6),(8,15),(10,3),(10,9),(12,25)]}\n",
        "    return (d.month, d.day) in BASIC_HOLI\n",
        "\n",
        "# =========================\n",
        "# í‰ê°€ ë©”íŠ¸ë¦­ (0 ì œì™¸ SMAPE)\n",
        "# =========================\n",
        "def smape_ignore_zero(y_true, y_pred):\n",
        "    mask = (y_true != 0)\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    yt = y_true[mask]\n",
        "    yp = y_pred[mask]\n",
        "    denom = (np.abs(yt) + np.abs(yp))\n",
        "    denom[denom == 0] = 1e-9\n",
        "    return np.mean(2.0 * np.abs(yt - yp) / denom)\n",
        "\n",
        "def lgb_smape_eval(y_pred, dataset):\n",
        "    y_true = dataset.get_label()\n",
        "    return 'smape_no0', smape_ignore_zero(y_true, y_pred), False\n",
        "\n",
        "# =========================\n",
        "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "# =========================\n",
        "def split_store_menu(x):\n",
        "    \"\"\"ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…ì„ ì˜ì—…ì¥ëª…ê³¼ ë©”ë‰´ëª…ìœ¼ë¡œ ë¶„ë¦¬\"\"\"\n",
        "    parts = str(x).split('_', 1)\n",
        "    if len(parts) == 1:\n",
        "        return parts[0], ''\n",
        "    return parts[0], parts[1]\n",
        "\n",
        "def ensure_daily_continuity(df):\n",
        "    \"\"\"ê²°ì¸¡ ë‚ ì§œë¥¼ 0ìœ¼ë¡œ ì±„ì›Œ ì¼ë³„ ì—°ì†ì„± ë³´ì¥\"\"\"\n",
        "    out = []\n",
        "    for k, g in df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', sort=False):\n",
        "        g = g.sort_values('ì˜ì—…ì¼ì').copy()\n",
        "        full = pd.DataFrame({\n",
        "            'ì˜ì—…ì¼ì': pd.date_range(g['ì˜ì—…ì¼ì'].min(), g['ì˜ì—…ì¼ì'].max(), freq='D')\n",
        "        })\n",
        "        full['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = k\n",
        "        full = full.merge(g, on=['ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'], how='left')\n",
        "        full['ë§¤ì¶œìˆ˜ëŸ‰'] = full['ë§¤ì¶œìˆ˜ëŸ‰'].fillna(0.0)\n",
        "        out.append(full)\n",
        "    return pd.concat(out, ignore_index=True)\n",
        "\n",
        "# =========================\n",
        "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§\n",
        "# =========================\n",
        "PEAK_MONTHS = {1,2,7,8,12}\n",
        "\n",
        "def add_calendar_features(df):\n",
        "    \"\"\"ë‹¬ë ¥ ê´€ë ¨ í”¼ì²˜ ì¶”ê°€\"\"\"\n",
        "    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])\n",
        "    df['year'] = df['ì˜ì—…ì¼ì'].dt.year\n",
        "    df['month'] = df['ì˜ì—…ì¼ì'].dt.month\n",
        "    df['day'] = df['ì˜ì—…ì¼ì'].dt.day\n",
        "    df['dow'] = df['ì˜ì—…ì¼ì'].dt.weekday  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼\n",
        "    df['week'] = df['ì˜ì—…ì¼ì'].dt.isocalendar().week.astype(int)\n",
        "    df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
        "    df['is_holiday'] = df['ì˜ì—…ì¼ì'].dt.date.map(lambda d: int(_is_holiday_date(d)))\n",
        "    df['is_pre_holiday'] = df['ì˜ì—…ì¼ì'].dt.date.map(lambda d: int(_is_holiday_date(d + timedelta(days=1))))\n",
        "    df['is_post_holiday'] = df['ì˜ì—…ì¼ì'].dt.date.map(lambda d: int(_is_holiday_date(d - timedelta(days=1))))\n",
        "    df['is_peak'] = df['month'].isin(PEAK_MONTHS).astype(int)\n",
        "    df['month_dow'] = df['month'].astype(str) + '_' + df['dow'].astype(str)\n",
        "    return df\n",
        "\n",
        "def add_ts_features(df, lags=(1,2,3,5,7,14,21,28), roll_windows=(3,7,14,21,28,56)):\n",
        "    \"\"\"ì‹œê³„ì—´ ë¼ê·¸ ë° ë¡¤ë§ í”¼ì²˜ ì¶”ê°€\"\"\"\n",
        "    df = df.sort_values(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).copy()\n",
        "    g_series = df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ë§¤ì¶œìˆ˜ëŸ‰']\n",
        "\n",
        "    # ë¼ê·¸ í”¼ì²˜\n",
        "    for L in lags:\n",
        "        df[f'lag_{L}'] = g_series.shift(L)\n",
        "\n",
        "    # ë¡¤ë§ í†µê³„ í”¼ì²˜\n",
        "    for W in roll_windows:\n",
        "        df[f'rmean_{W}'] = g_series.shift(1).rolling(W, min_periods=1).mean()\n",
        "        df[f'rmedian_{W}'] = g_series.shift(1).rolling(W, min_periods=1).median()\n",
        "        df[f'rstd_{W}'] = g_series.shift(1).rolling(W, min_periods=1).std()\n",
        "\n",
        "    # ìš”ì¼ë³„ í‰ê· \n",
        "    df['_prev'] = g_series.shift(1)\n",
        "    df['dow_mean_14'] = (\n",
        "        df.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','dow'])['_prev']\n",
        "          .transform(lambda s: s.rolling(14, min_periods=1).mean())\n",
        "    )\n",
        "    df.drop(columns=['_prev'], inplace=True)\n",
        "    return df\n",
        "\n",
        "# =========================\n",
        "# ë¼ë²¨ ì¸ì½”ë”©\n",
        "# =========================\n",
        "def fit_label_encoders(df):\n",
        "    \"\"\"ë¼ë²¨ ì¸ì½”ë” í•™ìŠµ\"\"\"\n",
        "    store, menu = zip(*df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].map(split_store_menu))\n",
        "    df['_store'] = store\n",
        "    df['_menu'] = menu\n",
        "\n",
        "    le_store = LabelEncoder().fit(df['_store'])\n",
        "    le_menu = LabelEncoder().fit(df['_menu'])\n",
        "    le_month_dow = LabelEncoder().fit(df['month_dow'])\n",
        "\n",
        "    return le_store, le_menu, le_month_dow\n",
        "\n",
        "def apply_label_encoders(df, le_store, le_menu, le_month_dow):\n",
        "    \"\"\"ë¼ë²¨ ì¸ì½”ë”© ì ìš©\"\"\"\n",
        "    store, menu = zip(*df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].map(split_store_menu))\n",
        "    df['_store'] = store\n",
        "    df['_menu'] = menu\n",
        "\n",
        "    df['_store_le'] = df['_store'].map(lambda x: le_store.transform([x])[0] if x in le_store.classes_ else -1)\n",
        "    df['_menu_le'] = df['_menu'].map(lambda x: le_menu.transform([x])[0] if x in le_menu.classes_ else -1)\n",
        "    df['_month_dow_le'] = df['month_dow'].map(lambda x: le_month_dow.transform([x])[0] if x in le_month_dow.classes_ else -1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========================\n",
        "# ì—°íšŒì¥ íƒì§€ ì‹œìŠ¤í…œ\n",
        "# =========================\n",
        "_DEFAULT_BANQUET_DICT = {\n",
        "    \"strong_regex_patterns\": [\n",
        "        r\"(ëŒ€|ì†Œ)?\\s*ì—°íšŒ(ì¥)?\",\n",
        "        r\"ì—°\\s*íšŒ\\s*ì¥\",\n",
        "        r\"ì›¨ë”©(\\s*í™€)?\",\n",
        "        r\"ì»¨ë²¤ì…˜(\\s*í™€)?\",\n",
        "        r\"(ê·¸ëœë“œ)?\\s*ë³¼ë£¸|ballroom\",\n",
        "        r\"banquet\",\n",
        "        r\"function\\s*room\",\n",
        "        r\"ëŒ€ê´€\",\n",
        "        r\"ì˜ˆì‹\",\n",
        "        r\"í–‰ì‚¬(\\s*ì¥)?\",\n",
        "    ],\n",
        "    \"whitelist_tokens\": [\n",
        "        \"ì—°íšŒ\",\"ì—°íšŒì¥\",\"ëŒ€ì—°íšŒ\",\"ì†Œì—°íšŒ\",\"ëŒ€ì—°íšŒì¥\",\"ì†Œì—°íšŒì¥\",\n",
        "        \"ì›¨ë”©\",\"ì›¨ë”©í™€\",\"ì»¨ë²¤ì…˜\",\"ì»¨ë²¤ì…˜í™€\",\"ë³¼ë£¸\",\"ê·¸ëœë“œë³¼ë£¸\",\n",
        "        \"banquet\",\"ballroom\",\"function\",\"ëŒ€ê´€\",\"ì˜ˆì‹\",\"í–‰ì‚¬\",\"ì—°íšŒì½”ìŠ¤\",\"ì—°íšŒì„¸íŠ¸\"\n",
        "    ],\n",
        "    \"blacklist_tokens\": [\n",
        "        \"í™€\",\"ë£¸\",\"ë£¸1\",\"ë£¸2\",\"ë£¸A\",\"ë£¸B\",\"ë£¸C\",\"í™€A\",\"í™€B\",\"í™€C\",\"ë£¸(ì†Œ)\",\"ë£¸(ëŒ€)\"\n",
        "    ],\n",
        "    \"notes\": \"ì •ê·œì‹ ìš°ì„  ì ìš©. ë‹¨ì¼ ìœ„í—˜ í† í°ì€ ì•ˆì „ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ í—ˆìš©\"\n",
        "}\n",
        "\n",
        "def load_banquet_dict(path=None):\n",
        "    \"\"\"ì—°íšŒì¥ íƒì§€ ì‚¬ì „ ë¡œë“œ\"\"\"\n",
        "    if path and os.path.exists(path):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ì—°íšŒì¥ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    return _DEFAULT_BANQUET_DICT\n",
        "\n",
        "def banquet_name_match(store, menu, dict_):\n",
        "    \"\"\"ì´ë¦„ ê¸°ë°˜ ì—°íšŒì¥ ë§¤ì¹­\"\"\"\n",
        "    s = str(store).lower()\n",
        "    m = str(menu).lower()\n",
        "    full_texts = [s, m, f\"{s}_{m}\"]\n",
        "\n",
        "    # ê°•í•œ ì •ê·œì‹ íŒ¨í„´ ì²´í¬\n",
        "    for pattern in dict_[\"strong_regex_patterns\"]:\n",
        "        regex = re.compile(pattern, re.IGNORECASE)\n",
        "        if any(regex.search(text or \"\") for text in full_texts):\n",
        "            return True\n",
        "\n",
        "    # í† í° ê¸°ë°˜ ì²´í¬\n",
        "    def tokenize(text):\n",
        "        return [t for t in re.split(r\"[^0-9A-Za-zê°€-í£]+\", str(text)) if t]\n",
        "\n",
        "    tokens = set(tokenize(s) + tokenize(m))\n",
        "\n",
        "    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬\n",
        "    if any(tok.lower() in [w.lower() for w in dict_[\"whitelist_tokens\"]] for tok in tokens):\n",
        "        return True\n",
        "\n",
        "    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ë§Œ ìˆìœ¼ë©´ False\n",
        "    if any(tok in dict_[\"blacklist_tokens\"] for tok in tokens):\n",
        "        return False\n",
        "\n",
        "    return False\n",
        "\n",
        "# ì—°íšŒì¥ ì„ê³„ê°’ (ê¸°ë³¸ê°’)\n",
        "DEFAULT_BANQUET_THRESHOLDS = {\n",
        "    \"var\": 2.16,\n",
        "    \"weekend\": 4.92,\n",
        "    \"holiday\": 4.92\n",
        "}\n",
        "\n",
        "def save_banquet_thresholds(thresholds, path):\n",
        "    \"\"\"ì—°íšŒì¥ ì„ê³„ê°’ ì €ì¥\"\"\"\n",
        "    try:\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(thresholds, f, ensure_ascii=False, indent=2)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ì„ê³„ê°’ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "def load_banquet_thresholds(path):\n",
        "    \"\"\"ì—°íšŒì¥ ì„ê³„ê°’ ë¡œë“œ\"\"\"\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ì„ê³„ê°’ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    return None\n",
        "\n",
        "def fit_banquet_thresholds_from_train(train_df):\n",
        "    \"\"\"í•™ìŠµ ë°ì´í„°ì—ì„œ ì—°íšŒì¥ í†µê³„ ì„ê³„ê°’ í•™ìŠµ\"\"\"\n",
        "    df = train_df.copy()\n",
        "    df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'])\n",
        "    df.loc[df['ë§¤ì¶œìˆ˜ëŸ‰'] < 0, 'ë§¤ì¶œìˆ˜ëŸ‰'] = 0.0\n",
        "    df['dow'] = df['ì˜ì—…ì¼ì'].dt.weekday\n",
        "    df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
        "    df['is_holiday'] = df['ì˜ì—…ì¼ì'].dt.date.map(lambda d: int(_is_holiday_date(d)))\n",
        "\n",
        "    records = []\n",
        "    for key, g in df.groupby(\"ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…\", sort=False):\n",
        "        g = g.sort_values(\"ì˜ì—…ì¼ì\")\n",
        "        if len(g) == 0:\n",
        "            continue\n",
        "\n",
        "        # ìµœê·¼ 28ì¼ ë°ì´í„° ì¶”ì¶œ\n",
        "        last_day = g['ì˜ì—…ì¼ì'].max()\n",
        "        recent = g[g['ì˜ì—…ì¼ì'] >= (last_day - pd.Timedelta(days=27))]\n",
        "        if len(recent) < 14:\n",
        "            recent = g.tail(28)\n",
        "\n",
        "        sales = recent['ë§¤ì¶œìˆ˜ëŸ‰'].astype(float).values\n",
        "        if len(sales) == 0:\n",
        "            continue\n",
        "\n",
        "        # ë³€ë™ì„± ë¹„ìœ¨\n",
        "        mean_all = np.nanmean(sales)\n",
        "        std_all = np.nanstd(sales)\n",
        "        var_ratio = std_all / (mean_all + 1e-6)\n",
        "\n",
        "        # ì£¼ë§ ë¹„ìœ¨\n",
        "        weekday_sales = recent[recent['is_weekend'] == 0]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        weekend_sales = recent[recent['is_weekend'] == 1]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        mean_weekday = np.nanmean(weekday_sales) if len(weekday_sales) > 0 else np.nan\n",
        "        mean_weekend = np.nanmean(weekend_sales) if len(weekend_sales) > 0 else np.nan\n",
        "        weekend_ratio = (mean_weekend / (mean_weekday + 1e-6)) if not np.isnan(mean_weekday) and not np.isnan(mean_weekend) else np.nan\n",
        "\n",
        "        # ê³µíœ´ì¼ ë¹„ìœ¨\n",
        "        holiday_sales = recent[recent['is_holiday'] == 1]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        non_holiday_sales = recent[recent['is_holiday'] == 0]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        mean_holiday = np.nanmean(holiday_sales) if len(holiday_sales) > 0 else np.nan\n",
        "        mean_non_holiday = np.nanmean(non_holiday_sales) if len(non_holiday_sales) > 0 else np.nan\n",
        "        holiday_ratio = (mean_holiday / (mean_non_holiday + 1e-6)) if not np.isnan(mean_holiday) and not np.isnan(mean_non_holiday) else np.nan\n",
        "\n",
        "        records.append((key, var_ratio, weekend_ratio, holiday_ratio))\n",
        "\n",
        "    stat_df = pd.DataFrame(records, columns=[\"name\",\"var_ratio\",\"weekend_ratio\",\"holiday_ratio\"])\n",
        "    clean_df = stat_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "    if len(clean_df) < 20:\n",
        "        return DEFAULT_BANQUET_THRESHOLDS\n",
        "\n",
        "    thresholds = {\n",
        "        \"var\": float(clean_df['var_ratio'].quantile(0.90)),\n",
        "        \"weekend\": float(clean_df['weekend_ratio'].quantile(0.90)),\n",
        "        \"holiday\": float(clean_df['holiday_ratio'].quantile(0.90)),\n",
        "    }\n",
        "    return thresholds\n",
        "\n",
        "def banquet_stats_match(group_df, thresholds):\n",
        "    \"\"\"í†µê³„ ê¸°ë°˜ ì—°íšŒì¥ ë§¤ì¹­\"\"\"\n",
        "    g = group_df.sort_values('ì˜ì—…ì¼ì')\n",
        "    if len(g) < 14:\n",
        "        return False\n",
        "\n",
        "    # ìµœê·¼ ë°ì´í„° ì¶”ì¶œ\n",
        "    last_day = g['ì˜ì—…ì¼ì'].max()\n",
        "    recent = g[g['ì˜ì—…ì¼ì'] >= (last_day - pd.Timedelta(days=27))].copy()\n",
        "    if len(recent) < 14:\n",
        "        recent = g.tail(28).copy()\n",
        "\n",
        "    sales = recent['ë§¤ì¶œìˆ˜ëŸ‰'].values.astype(float)\n",
        "    mean_all = np.nanmean(sales)\n",
        "    std_all = np.nanstd(sales)\n",
        "    var_ratio = std_all / (mean_all + 1e-6)\n",
        "    cond_var = (var_ratio > thresholds.get(\"var\", 2.16))\n",
        "\n",
        "    # ì£¼ë§ ì¡°ê±´\n",
        "    cond_weekend = False\n",
        "    if 'is_weekend' in recent.columns:\n",
        "        weekday_sales = recent[recent['is_weekend'] == 0]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        weekend_sales = recent[recent['is_weekend'] == 1]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        if len(weekday_sales) > 0 and len(weekend_sales) > 0:\n",
        "            mean_weekday = np.nanmean(weekday_sales)\n",
        "            mean_weekend = np.nanmean(weekend_sales)\n",
        "            if not np.isnan(mean_weekday) and not np.isnan(mean_weekend):\n",
        "                weekend_ratio = mean_weekend / max(mean_weekday, 1e-6)\n",
        "                cond_weekend = weekend_ratio > thresholds.get(\"weekend\", 4.92)\n",
        "\n",
        "    # ê³µíœ´ì¼ ì¡°ê±´\n",
        "    cond_holiday = False\n",
        "    if 'is_holiday' in recent.columns:\n",
        "        holiday_sales = recent[recent['is_holiday'] == 1]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        non_holiday_sales = recent[recent['is_holiday'] == 0]['ë§¤ì¶œìˆ˜ëŸ‰'].values\n",
        "        if len(holiday_sales) > 0 and len(non_holiday_sales) > 0:\n",
        "            mean_holiday = np.nanmean(holiday_sales)\n",
        "            mean_non_holiday = np.nanmean(non_holiday_sales)\n",
        "            if not np.isnan(mean_holiday) and not np.isnan(mean_non_holiday):\n",
        "                holiday_ratio = mean_holiday / max(mean_non_holiday, 1e-6)\n",
        "                cond_holiday = holiday_ratio > thresholds.get(\"holiday\", 4.92)\n",
        "\n",
        "    # 3ê°œ ì¡°ê±´ ì¤‘ 2ê°œ ì´ìƒ ë§Œì¡±\n",
        "    return (int(cond_var) + int(cond_weekend) + int(cond_holiday)) >= 2\n",
        "\n",
        "def decide_banquet_for_group(key, group_df, banquet_dict, thresholds):\n",
        "    \"\"\"ê·¸ë£¹ë³„ ì—°íšŒì¥ ì—¬ë¶€ ê²°ì • (ì´ë¦„ + í†µê³„ í•˜ì´ë¸Œë¦¬ë“œ)\"\"\"\n",
        "    store, menu = split_store_menu(key)\n",
        "\n",
        "    # ì´ë¦„ ê¸°ë°˜ ë§¤ì¹­ì´ ìš°ì„ \n",
        "    name_match = banquet_name_match(store, menu, banquet_dict)\n",
        "    if name_match:\n",
        "        return 1\n",
        "\n",
        "    # í†µê³„ ê¸°ë°˜ ë§¤ì¹­\n",
        "    stats_match = banquet_stats_match(group_df, thresholds)\n",
        "    return int(stats_match)\n",
        "\n",
        "def add_banquet_flag(df, banquet_dict, thresholds):\n",
        "    \"\"\"ì—°íšŒì¥ í”Œë˜ê·¸ ì¶”ê°€\"\"\"\n",
        "    flags = []\n",
        "\n",
        "    # ìº˜ë¦°ë” í”¼ì²˜ê°€ ì—†ìœ¼ë©´ ì¶”ê°€\n",
        "    if 'dow' not in df.columns or 'is_weekend' not in df.columns or 'is_holiday' not in df.columns:\n",
        "        df_tmp = add_calendar_features(df.copy())\n",
        "    else:\n",
        "        df_tmp = df.copy()\n",
        "\n",
        "    for key, group in df_tmp.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', sort=False):\n",
        "        flag = decide_banquet_for_group(key, group, banquet_dict, thresholds)\n",
        "        flags.append((key, flag))\n",
        "\n",
        "    flag_df = pd.DataFrame(flags, columns=['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', 'is_banquet'])\n",
        "    return df.merge(flag_df, on='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', how='left')\n",
        "\n",
        "# =========================\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "# =========================\n",
        "def train_ensemble_models(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ\"\"\"\n",
        "    models = []\n",
        "\n",
        "    # ëª¨ë¸ 1\n",
        "    params1 = {\n",
        "        'objective': 'poisson',\n",
        "        'metric': 'None',\n",
        "        'learning_rate': 0.025,\n",
        "        'num_leaves': 640,\n",
        "        'max_depth': -1,\n",
        "        'min_data_in_leaf': 80,\n",
        "        'feature_fraction': 0.90,\n",
        "        'bagging_fraction': 0.90,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 0.0,\n",
        "        'lambda_l2': 0.1,\n",
        "        'verbose': -1,\n",
        "        'seed': SEED,\n",
        "        'force_row_wise': True\n",
        "    }\n",
        "\n",
        "    model1 = lgb.train(\n",
        "        params1,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        num_boost_round=9000,\n",
        "        valid_sets=[lgb.Dataset(X_val, label=y_val)],\n",
        "        feval=lgb_smape_eval,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=400, verbose=False)]\n",
        "    )\n",
        "    models.append(model1)\n",
        "\n",
        "    # ëª¨ë¸ 2\n",
        "    params2 = {\n",
        "        'objective': 'poisson',\n",
        "        'metric': 'None',\n",
        "        'learning_rate': 0.03,\n",
        "        'num_leaves': 512,\n",
        "        'max_depth': -1,\n",
        "        'min_data_in_leaf': 100,\n",
        "        'feature_fraction': 0.85,\n",
        "        'bagging_fraction': 0.90,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 0.1,\n",
        "        'lambda_l2': 0.1,\n",
        "        'verbose': -1,\n",
        "        'seed': SEED + 1,\n",
        "        'force_row_wise': True\n",
        "    }\n",
        "\n",
        "    model2 = lgb.train(\n",
        "        params2,\n",
        "        lgb.Dataset(X_train, label=y_train),\n",
        "        num_boost_round=9000,\n",
        "        valid_sets=[lgb.Dataset(X_val, label=y_val)],\n",
        "        feval=lgb_smape_eval,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=300, verbose=False)]\n",
        "    )\n",
        "    models.append(model2)\n",
        "\n",
        "    return models\n",
        "\n",
        "# =========================\n",
        "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "# =========================\n",
        "def prepare_features_block(df_block, le_store, le_menu, le_month_dow, banquet_dict, thresholds):\n",
        "    \"\"\"ì˜ˆì¸¡ìš© í”¼ì²˜ ë¸”ë¡ ì¤€ë¹„\"\"\"\n",
        "    df_block = add_calendar_features(df_block)\n",
        "    df_block = add_banquet_flag(df_block, banquet_dict, thresholds)\n",
        "    df_block = add_ts_features(df_block)\n",
        "    df_block = apply_label_encoders(df_block, le_store, le_menu, le_month_dow)\n",
        "    return df_block\n",
        "\n",
        "def one_step_predict(series_df, predict_date, model, features, le_store, le_menu, le_month_dow, banquet_dict, thresholds):\n",
        "    \"\"\"1ì¼ ì• ì˜ˆì¸¡\"\"\"\n",
        "    tmp = series_df.copy()\n",
        "    new_row = pd.DataFrame({\n",
        "        'ì˜ì—…ì¼ì': [predict_date],\n",
        "        'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': [series_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].iloc[0]],\n",
        "        'ë§¤ì¶œìˆ˜ëŸ‰': [np.nan]\n",
        "    })\n",
        "    tmp = pd.concat([tmp, new_row], ignore_index=True)\n",
        "    tmp = prepare_features_block(tmp, le_store, le_menu, le_month_dow, banquet_dict, thresholds)\n",
        "\n",
        "    row = tmp[tmp['ì˜ì—…ì¼ì'] == predict_date].iloc[-1]\n",
        "    x = row[features].values.reshape(1, -1)\n",
        "    pred = model.predict(x, num_iteration=getattr(model, \"best_iteration\", None))[0]\n",
        "    return max(0.0, float(pred))\n",
        "\n",
        "def forecast_7days_for_file(test_df, model_selector, features, le_store, le_menu, le_month_dow, banquet_dict, thresholds, test_prefix):\n",
        "    \"\"\"í…ŒìŠ¤íŠ¸ íŒŒì¼ì— ëŒ€í•œ 7ì¼ ì˜ˆì¸¡\"\"\"\n",
        "    test_df = test_df.copy()\n",
        "    test_df['ì˜ì—…ì¼ì'] = pd.to_datetime(test_df['ì˜ì—…ì¼ì'])\n",
        "    test_df.loc[test_df['ë§¤ì¶œìˆ˜ëŸ‰'] < 0, 'ë§¤ì¶œìˆ˜ëŸ‰'] = 0.0\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for key, group in tqdm(test_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…', sort=False), desc=f\"Forecasting {test_prefix}\", leave=False):\n",
        "        group = group.sort_values('ì˜ì—…ì¼ì').copy()\n",
        "\n",
        "        # ì—°ì†ì„± ë³´ì¥\n",
        "        full_range = pd.DataFrame({\n",
        "            'ì˜ì—…ì¼ì': pd.date_range(group['ì˜ì—…ì¼ì'].min(), group['ì˜ì—…ì¼ì'].max(), freq='D')\n",
        "        })\n",
        "        full_range['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = key\n",
        "        full_range = full_range.merge(group, on=['ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'], how='left')\n",
        "        full_range['ë§¤ì¶œìˆ˜ëŸ‰'] = full_range['ë§¤ì¶œìˆ˜ëŸ‰'].fillna(0.0)\n",
        "\n",
        "        # ì—°íšŒì¥ ì—¬ë¶€ íŒë‹¨ ë° ëª¨ë¸ ì„ íƒ\n",
        "        banquet_flag = decide_banquet_for_group(key, add_calendar_features(full_range.copy()), banquet_dict, thresholds)\n",
        "\n",
        "        if isinstance(model_selector, dict):  # separate ëª¨ë“œ\n",
        "            models = model_selector['banquet'] if banquet_flag else model_selector['non']\n",
        "        else:  # together ëª¨ë“œ\n",
        "            models = model_selector\n",
        "\n",
        "        # 7ì¼ ì˜ˆì¸¡\n",
        "        last_day = full_range['ì˜ì—…ì¼ì'].max()\n",
        "        current_data = full_range[['ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰']].copy()\n",
        "\n",
        "        daily_predictions = []\n",
        "        for horizon in range(1, 8):\n",
        "            prediction_date = last_day + pd.Timedelta(days=horizon)\n",
        "\n",
        "            # ì•™ìƒë¸” ì˜ˆì¸¡\n",
        "            ensemble_pred = np.mean([\n",
        "                one_step_predict(current_data, prediction_date, model, features, le_store, le_menu, le_month_dow, banquet_dict, thresholds)\n",
        "                for model in models\n",
        "            ])\n",
        "\n",
        "            daily_predictions.append(ensemble_pred)\n",
        "\n",
        "            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì˜ˆì¸¡ì„ ìœ„í•´ ì¶”ê°€\n",
        "            new_row = pd.DataFrame({\n",
        "                'ì˜ì—…ì¼ì': [prediction_date],\n",
        "                'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…': [key],\n",
        "                'ë§¤ì¶œìˆ˜ëŸ‰': [ensemble_pred]\n",
        "            })\n",
        "            current_data = pd.concat([current_data, new_row], ignore_index=True)\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        for horizon, pred in enumerate(daily_predictions, 1):\n",
        "            predictions.append([key, test_prefix, horizon, pred])\n",
        "\n",
        "    result_df = pd.DataFrame(predictions, columns=['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','test_prefix','offset','ë§¤ì¶œìˆ˜ëŸ‰'])\n",
        "    return result_df\n",
        "    # =========================\n",
        "# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„\n",
        "# =========================\n",
        "# í•™ìŠµ ì§„í–‰ë¥ ì„ ë³´ì—¬ì£¼ëŠ” LightGBMìš© ì½œë°± í•¨ìˆ˜\n",
        "def create_lgbm_tqdm_callback(num_boost_round):\n",
        "    \"\"\"Create a tqdm callback for LightGBM training.\"\"\"\n",
        "    pbar = tqdm(total=num_boost_round, desc=\"ğŸš€ Training\", leave=False)\n",
        "\n",
        "    def callback(env):\n",
        "        current_iter = env.iteration\n",
        "        best_iter = getattr(env, \"best_iteration\", -1)\n",
        "\n",
        "        # update the progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({\n",
        "            'Current Iter': current_iter,\n",
        "            'Best Iter': best_iter if best_iter != -1 else 'N/A'\n",
        "        })\n",
        "\n",
        "        # check for early stopping and close pbar\n",
        "        if env.early_stopping_round is not None and current_iter >= env.best_iteration:\n",
        "            pbar.close()\n",
        "\n",
        "    return callback\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"ğŸš€ ì—°íšŒì¥ ë¶„ë¦¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘\")\n",
        "\n",
        "    # 1. ì—°íšŒì¥ íƒì§€ ì„¤ì • ë¡œë“œ\n",
        "    banquet_dict = load_banquet_dict(BANQUET_DICT_PATH)\n",
        "    print(f\"ğŸ“– ì—°íšŒì¥ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (íŒ¨í„´ {len(banquet_dict['strong_regex_patterns'])}ê°œ)\")\n",
        "\n",
        "    # 2. í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "    print(\"ğŸ“‚ í•™ìŠµ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
        "    train = pd.read_csv(TRAIN_PATH)\n",
        "    train['ì˜ì—…ì¼ì'] = pd.to_datetime(train['ì˜ì—…ì¼ì'])\n",
        "    train.loc[train['ë§¤ì¶œìˆ˜ëŸ‰'] < 0, 'ë§¤ì¶œìˆ˜ëŸ‰'] = 0.0\n",
        "    print(f\"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(train):,} í–‰\")\n",
        "\n",
        "    # 3. ì—°íšŒì¥ ì„ê³„ê°’ ì„¤ì •\n",
        "    loaded_thresholds = load_banquet_thresholds(BANQUET_THRESH_PATH)\n",
        "    if loaded_thresholds is None:\n",
        "        print(\"ğŸ” í•™ìŠµ ë°ì´í„°ì—ì„œ ì—°íšŒì¥ ì„ê³„ê°’ ì¶”ì • ì¤‘...\")\n",
        "        banquet_thresholds = fit_banquet_thresholds_from_train(train)\n",
        "        save_banquet_thresholds(banquet_thresholds, BANQUET_THRESH_PATH)\n",
        "        print(f\"âœ… ì„ê³„ê°’ ì¶”ì • ì™„ë£Œ - Var: {banquet_thresholds['var']:.3f}, Weekend: {banquet_thresholds['weekend']:.3f}, Holiday: {banquet_thresholds['holiday']:.3f}\")\n",
        "    else:\n",
        "        banquet_thresholds = loaded_thresholds\n",
        "        print(f\"âœ… ê¸°ì¡´ ì—°íšŒì¥ ì„ê³„ê°’ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "    # 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° ë¶„í• \n",
        "    print(\"ğŸ› ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° ë¶„í•  ì¤‘...\")\n",
        "\n",
        "    # í•™ìŠµ ë°ì´í„°ì— ì¼ë³„ ì—°ì†ì„± ë³´ì¥ ë° í”¼ì²˜ ì¶”ê°€\n",
        "    train_full = ensure_daily_continuity(train)\n",
        "    train_full = add_calendar_features(train_full)\n",
        "    train_full = add_banquet_flag(train_full, banquet_dict, banquet_thresholds)\n",
        "    train_full = add_ts_features(train_full)\n",
        "\n",
        "    # ë¼ë²¨ ì¸ì½”ë” í•™ìŠµ\n",
        "    le_store, le_menu, le_month_dow = fit_label_encoders(train_full)\n",
        "    train_full = apply_label_encoders(train_full, le_store, le_menu, le_month_dow)\n",
        "\n",
        "    # í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° ë¶„í• \n",
        "    TRAIN_CUTOFF = pd.to_datetime('2024-03-31')\n",
        "    train_df = train_full[train_full['ì˜ì—…ì¼ì'] <= TRAIN_CUTOFF]\n",
        "    val_df = train_full[train_full['ì˜ì—…ì¼ì'] > TRAIN_CUTOFF].copy()\n",
        "\n",
        "    # í”¼ì²˜ ë° íƒ€ê²Ÿ ì„¤ì •\n",
        "    TARGET = 'ë§¤ì¶œìˆ˜ëŸ‰'\n",
        "    FEATURES = [\n",
        "        'year','month','day','dow','week',\n",
        "        'is_weekend','is_holiday','is_pre_holiday','is_post_holiday',\n",
        "        'is_peak', 'is_banquet',\n",
        "        '_store_le','_menu_le','_month_dow_le',\n",
        "    ]\n",
        "    FEATURES.extend([f'lag_{L}' for L in (1,2,3,5,7,14,21,28)])\n",
        "    FEATURES.extend([f'rmean_{W}' for W in (3,7,14,21,28,56)])\n",
        "    FEATURES.extend([f'rmedian_{W}' for W in (3,7,14,21,28,56)])\n",
        "    FEATURES.extend([f'rstd_{W}' for W in (3,7,14,21,28,56)])\n",
        "    FEATURES.append('dow_mean_14')\n",
        "\n",
        "    # ëˆ„ë½ëœ ë°ì´í„° ì œê±°\n",
        "    X_train = train_df.dropna(subset=FEATURES).reset_index(drop=True)[FEATURES]\n",
        "    y_train = train_df.dropna(subset=FEATURES).reset_index(drop=True)[TARGET]\n",
        "    X_val = val_df[FEATURES]\n",
        "    y_val = val_df[TARGET]\n",
        "\n",
        "    print(\"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ\")\n",
        "    print(f\"   í•™ìŠµ ë°ì´í„°: {len(X_train):,}ê°œ, ê²€ì¦ ë°ì´í„°: {len(X_val):,}ê°œ\")\n",
        "\n",
        "    # 5. ëª¨ë¸ í•™ìŠµ (ì—°íšŒì¥ ë¶„ë¦¬ ëª¨ë“œ)\n",
        "    models = {}\n",
        "    if BANQUET_MODE == \"separate\":\n",
        "        print(\"ğŸ¤– ì—°íšŒì¥ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "\n",
        "        # ì—°íšŒì¥ ê·¸ë£¹ ë¶„ë¦¬\n",
        "        banquet_train_df = train_df[train_df['is_banquet'] == 1].dropna(subset=FEATURES).reset_index(drop=True)\n",
        "        non_banquet_train_df = train_df[train_df['is_banquet'] == 0].dropna(subset=FEATURES).reset_index(drop=True)\n",
        "\n",
        "        banquet_val_df = val_df[val_df['is_banquet'] == 1]\n",
        "        non_banquet_val_df = val_df[val_df['is_banquet'] == 0]\n",
        "\n",
        "        # ì—°íšŒì¥ ëª¨ë¸ í•™ìŠµ\n",
        "        print(f\"   - ì—°íšŒì¥ ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(banquet_train_df):,}ê°œ ë°ì´í„°)\")\n",
        "        if len(banquet_train_df) > 100:\n",
        "            models['banquet'] = train_ensemble_models(\n",
        "                banquet_train_df[FEATURES], banquet_train_df[TARGET],\n",
        "                banquet_val_df[FEATURES], banquet_val_df[TARGET]\n",
        "            )\n",
        "        else:\n",
        "            print(\"     âš ï¸ ì—°íšŒì¥ ë°ì´í„° ë¶€ì¡±, ì „ì²´ ëª¨ë¸ë¡œ í†µí•© í•™ìŠµí•©ë‹ˆë‹¤.\")\n",
        "            models['banquet'] = train_ensemble_models(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # ì¼ë°˜ ëª¨ë¸ í•™ìŠµ\n",
        "        print(f\"   - ì¼ë°˜ ëª¨ë¸ í•™ìŠµ ì¤‘... ({len(non_banquet_train_df):,}ê°œ ë°ì´í„°)\")\n",
        "        models['non'] = train_ensemble_models(\n",
        "            non_banquet_train_df[FEATURES], non_banquet_train_df[TARGET],\n",
        "            non_banquet_val_df[FEATURES], non_banquet_val_df[TARGET]\n",
        "        )\n",
        "        print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "\n",
        "    else: # together ëª¨ë“œ\n",
        "        print(\"ğŸ¤– ì „ì²´ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "        models['all'] = train_ensemble_models(X_train, y_train, X_val, y_val)\n",
        "        print(\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "\n",
        "    # 6. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì˜ˆì¸¡\n",
        "    print(\"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ë° ì˜ˆì¸¡ ì‹œì‘...\")\n",
        "    submission = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "    test_files = [f for f in os.listdir(TEST_DIR) if f.endswith('.csv')]\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    # ì „ì²´ í…ŒìŠ¤íŠ¸ íŒŒì¼ì— ëŒ€í•œ ì˜ˆì¸¡ ì§„í–‰\n",
        "    for test_file in tqdm(test_files, desc=\"Processing Test Files\"):\n",
        "        test_prefix = test_file.replace('.csv', '')\n",
        "        test_path = os.path.join(TEST_DIR, test_file)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "\n",
        "        if BANQUET_MODE == \"separate\":\n",
        "            test_models = {\n",
        "                'banquet': models.get('banquet'),\n",
        "                'non': models.get('non')\n",
        "            }\n",
        "        else:\n",
        "            test_models = models.get('all')\n",
        "\n",
        "        file_predictions = forecast_7days_for_file(\n",
        "            test_df=test_df,\n",
        "            model_selector=test_models,\n",
        "            features=FEATURES,\n",
        "            le_store=le_store,\n",
        "            le_menu=le_menu,\n",
        "            le_month_dow=le_month_dow,\n",
        "            banquet_dict=banquet_dict,\n",
        "            thresholds=banquet_thresholds,\n",
        "            test_prefix=test_prefix\n",
        "        )\n",
        "        all_predictions.append(file_predictions)\n",
        "        gc.collect() # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "\n",
        "    print(\"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ íŒŒì¼ ì˜ˆì¸¡ ì™„ë£Œ\")\n",
        "\n",
        "# 7. ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "final_pred_df = pd.concat(all_predictions, ignore_index=True)\n",
        "\n",
        "# ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„: ì™€ì´ë“œ í¬ë§·ì˜ ì œì¶œ íŒŒì¼ì— ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "print(\"âœï¸ ì œì¶œ íŒŒì¼ ì‘ì„± ì¤‘...\")\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ì˜ ì¸ë±ìŠ¤ë¥¼ 'ì˜ì—…ì¼ì'ë¡œ ì„¤ì •\n",
        "submission.set_index('ì˜ì—…ì¼ì', inplace=True)\n",
        "\n",
        "for _, row in tqdm(final_pred_df.iterrows(), total=len(final_pred_df), desc=\"Filling Submission\"):\n",
        "    store_menu = row['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']\n",
        "    test_prefix = row['test_prefix']\n",
        "    offset = row['offset']\n",
        "    prediction = row['ë§¤ì¶œìˆ˜ëŸ‰']\n",
        "\n",
        "    # 'TEST_01+1ì¼', 'TEST_01+2ì¼' í˜•ì‹ì˜ ì¸ë±ìŠ¤ ì´ë¦„ ìƒì„±\n",
        "    index_name = f\"{test_prefix}+{offset}ì¼\"\n",
        "\n",
        "    # í•´ë‹¹ ìœ„ì¹˜ì— ì˜ˆì¸¡ê°’ í• ë‹¹\n",
        "    if index_name in submission.index and store_menu in submission.columns:\n",
        "        submission.loc[index_name, store_menu] = max(0, int(round(prediction)))\n",
        "\n",
        "submission.reset_index(inplace=True)\n",
        "submission.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "print(f\"ğŸ‰ ì œì¶œ íŒŒì¼ì´ {OUTPUT_PATH}ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ]
}